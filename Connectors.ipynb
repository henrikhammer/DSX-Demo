{
    "cells": [
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Load data into a notebook using different Data Connectors"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Before you can start analyzing your data, you have to load the data from different data source. This reference notebook shows you how to use Data Connectors to load data that is stored in different data sources.\n\nThe notebook sample code shows you how to use Data Connectors to load data into a Python notebook. You can copy and paste these code snippets into the notebook you are developing.\n"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from ingest import Connectors\n", 
            "execution_count": 1
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "traceback": [
                        "\u001b[0;31m\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-7-96681b0453d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'ingest' is not defined"
                    ], 
                    "output_type": "error", 
                    "evalue": "name 'ingest' is not defined", 
                    "ename": "NameError"
                }
            ], 
            "source": "dir(ingest)", 
            "execution_count": 7
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Load data from Amazon S3"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "from ingest import Connectors\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\nS3loadoptions = { \n                  Connectors.AmazonS3.ACCESS_KEY          : '***********',\n                  Connectors.AmazonS3.SECRET_KEY          : '***********',\n                  Connectors.AmazonS3.SOURCE_BUCKET       : '***********',\n                  Connectors.AmazonS3.SOURCE_FILE_NAME    : 'addresses3.csv',\n                  Connectors.AmazonS3.SOURCE_INFER_SCHEMA : '1',\n                  Connectors.AmazonS3.SOURCE_FILE_FORMAT  : 'csv'}\n\nS3DF = sqlContext.read.format('com.ibm.spark.discover').options(**S3loadoptions).load()\nS3DF.printSchema()\nS3DF.show(5)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Now your dataset which was stored in Amazon S3 is loaded into a notebook as a DataFrame and you can begin analyzing it."
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Save the DataFrame to Amazon S3"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Once we have Analyzed the DataFrame and Transformed the Dataframe , User has the option to save the modified DataFrame back to Amazon S3 using the below code . In our sample code we are assuming that above DataFrame S3DF is the new Transformed Dataset that we need to save it back to Amazon S3."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "S3saveoptions = { Connectors.AmazonS3.ACCESS_KEY        : '***********',\n                  Connectors.AmazonS3.SECRET_KEY        : '***********',\n                  Connectors.AmazonS3.TARGET_BUCKET     : '***********',\n                  Connectors.AmazonS3.TARGET_FILE_NAME  : 'addresses4.csv',\n                  Connectors.AmazonS3.TARGET_WRITE_MODE : 'write'}\n\nNewS3DF = S3DF.write.format(\"com.ibm.spark.discover\").options(**S3saveoptions).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Load data from dashDB"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "dashDBloadOptions = { Connectors.DASHDB.HOST              : '***********',\n                      Connectors.DASHDB.DATABASE          : 'BLUDB',\n                      Connectors.DASHDB.USERNAME          : '***********',\n                      Connectors.DASHDB.PASSWORD          : '***********',\n                      Connectors.DASHDB.SOURCE_TABLE_NAME : 'DASH105036.TABLE1'}\n\ndashdbDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**dashDBloadOptions).load()\ndashdbDF.printSchema()\ndashdbDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Save the DataFrame to dashDB"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Once we have Analyzed the DataFrame and Transformed the Dataframe , User has the option to save the modified DataFrame back to dashDB using the below code . In our sample code we are assuming that above DataFrame dashdbDF is the new Transformed Dataset that we need to save it back to dashDB.\n"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "dashdbsaveoption = { \n                     Connectors.DASHDB.HOST              : '***********',\n                     Connectors.DASHDB.DATABASE          : 'BLUDB',\n                     Connectors.DASHDB.USERNAME          : '***********',\n                     Connectors.DASHDB.PASSWORD          : '***********',\n                     Connectors.DASHDB.TARGET_TABLE_NAME : 'DASH105036.TABLE2',\n                     Connectors.DASHDB.TARGET_WRITE_MODE : 'merge' }\n\nNewdashDBDF = dashdbDF.write.format(\"com.ibm.spark.discover\").options(**dashdbsaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Load data from Softlayer Objectstore"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "softlayerobjloadoptions = { \n    Connectors.SoftLayerObjectStorage.ACCESS_KEY          : '***********',\n    Connectors.SoftLayerObjectStorage.SECRET_KEY          : '***********',\n    Connectors.SoftLayerObjectStorage.URL                 : '***********',\n    Connectors.SoftLayerObjectStorage.SOURCE_CONTAINER    : '***********',\n    Connectors.SoftLayerObjectStorage.SOURCE_FILE_NAME    : 'users.avro', \n    Connectors.SoftLayerObjectStorage.SOURCE_FILE_FORMAT  : 'avro', \n    Connectors.SoftLayerObjectStorage.SOURCE_INFER_SCHEMA : '1'  }\n    \nsoftlyobjDF =  sqlContext.read.format(\"com.ibm.spark.discover\").options(**softlayerobjloadoptions).load()\nsoftlyobjDF.printSchema()\nsoftlyobjDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Save the DataFrame to Softlayer Objectstore "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Once we have Analyzed and Transformed the Dataframe , User has the option to save the modified DataFrame back to Softlayer Objectstore using the below code . In our sample code we are assuming that above DataFrame softlyobjDF is the new Transformed Dataset that we need to save it back to Softlayer Objectstore."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "softlayerobjsaveoptions = {\n    Connectors.SoftLayerObjectStorage.ACCESS_KEY         : '***********', \n    Connectors.SoftLayerObjectStorage.SECRET_KEY         : '***********', \n    Connectors.SoftLayerObjectStorage.URL                : '***********', \n    Connectors.SoftLayerObjectStorage.TARGET_CONTAINER   : '***********', \n    Connectors.SoftLayerObjectStorage.TARGET_FILE_NAME   : 'newusers.avro', \n    Connectors.SoftLayerObjectStorage.TARGET_FILE_FORMAT : 'avro', \n    Connectors.SoftLayerObjectStorage.TARGET_WRITE_MODE  : 'write'}\n    \nNewsoftlyobjDF = softlyobjDF.write.format(\"com.ibm.spark.discover\").options(**softlayerobjsaveoptions).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Load data from Cloudant"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "Cloudantloadoptions = {\n    Connectors.Cloudant.HOST            : '***********', \n    Connectors.Cloudant.PORT            : '443', \n    Connectors.Cloudant.SSL             : 'yes', \n    Connectors.Cloudant.SOURCE_DATABASE : 'sample' , \n    Connectors.Cloudant.USERNAME        : '***********' ,\n    Connectors.Cloudant.PASSWORD        : '***********'}\n\ncloudantDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**Cloudantloadoptions).load()\ncloudantDF.printSchema()\ncloudantDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Save the DataFrame to Cloudant"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Once we have Analyzed the DataFrame and Transformed the Dataframe , User has the option to save the modified DataFrame back to Cloudant using the below code . In our sample code we are assuming that above DataFrame cloudantDF is the new Transformed Dataset that we need to save it back to cloudant."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "Cloudantsaveoptions = {\n    Connectors.Cloudant.HOST                 : '***********',\n    Connectors.Cloudant.PORT                 : '443',\n    Connectors.Cloudant.SSL                  : 'yes',\n    Connectors.Cloudant.TARGET_DATABASE      : 'sample',\n    Connectors.Cloudant.USERNAME             : '***********',\n    Connectors.Cloudant.PASSWORD             : '***********',\n    Connectors.Cloudant.TARGET_DOCUMENT_TYPE : 'json'}\n\nNewcloudantDF = cloudantDF.write.format(\"com.ibm.spark.discover\").options(**Cloudantsaveoptions).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Load data from Bluemix Objectstore "
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "objectstoreloadOptions = {\n       Connectors.BluemixObjectStorage.AUTH_URL             : '***********',\n        Connectors.BluemixObjectStorage.USERID              : '***********',\n        Connectors.BluemixObjectStorage.PASSWORD            :'***********',\n        Connectors.BluemixObjectStorage.PROJECTID           : '***********',\n        Connectors.BluemixObjectStorage.REGION              : 'dallas',\n        Connectors.BluemixObjectStorage.SOURCE_CONTAINER    : 'newstage1',\n        Connectors.BluemixObjectStorage.SOURCE_FILE_NAME    : 'BlocPower_T.csv',\n        Connectors.BluemixObjectStorage.SOURCE_INFER_SCHEMA : '1'}\n                     \nobjectstoreDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**objectstoreloadOptions).load()\nobjectstoreDF.printSchema()\nobjectstoreDF.show(5)", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Save the DataFrame to Bluemix Objectstore"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Once we have Analyzed the DataFrame and Transformed the Dataframe , User has the option to save the modified DataFrame back to Bluemix Objectstore using the below code . In our sample code we are assuming that above DataFrame objectstoreDF is the new Transformed Dataset that we need to save it back to Bluemix Objectstore."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "objectstoresaveOptions = {\n        Connectors.BluemixObjectStorage.AUTH_URL          : '***********',\n        Connectors.BluemixObjectStorage.USERID            : '***********',\n        Connectors.BluemixObjectStorage.PASSWORD          : '***********',\n        Connectors.BluemixObjectStorage.PROJECTID         : '***********',\n        Connectors.BluemixObjectStorage.REGION            : 'dallas',\n        Connectors.BluemixObjectStorage.TARGET_CONTAINER  :  'newstage1',\n        Connectors.BluemixObjectStorage.TARGET_FILE_NAME  : 'NewBlocPower_T.csv',\n        Connectors.BluemixObjectStorage.TARGET_WRITE_MODE : 'write'}\n                     \nNewobjectstoreDF = objectstoreDF.write.format(\"com.ibm.spark.discover\").options(**objectstoresaveOptions).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Load Data from LocalFS "
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "If you happen to upload the files to the notebook environment and if we want to load that dataset in your scala notebook then we can use the below code"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "localfsloadoptions = {\n      Connectors.LocalFS.SOURCE_FILE_NAME    : '/gpfs/global_fs01/cluster/ys1-dwspark-dal09-env4-011.bluemix.net/user/sc93-b1ddac2d470956-05b1d10fb12b/data/test.csv',\n      Connectors.LocalFS.SOURCE_INFER_SCHEMA : '1',\n      Connectors.LocalFS.SOURCE_FILE_FORMAT  : 'csv'}\n\nLocalfsDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**localfsloadoptions).load()\nLocalfsDF.printSchema()\nLocalfsDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Save the DataFrame to LocalFS"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "Notebook Users has the option to save the DataFrame to LocalFS path using the below code . In our sample code we are assuming that above DataFrame LocalfsDF is the DataFrame that we want to save to LocalFS."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "localfssaveoptions = {\n      Connectors.LocalFS.TARGET_FILE_NAME : '/gpfs/global_fs01/cluster/ys1-dwspark-dal09-env4-011.bluemix.net/user/sc93-b1ddac2d470956-05b1d10fb12b/data/test.csv',\n      Connectors.LocalFS.TARGET_WRITE_MODE : 'write',\n      Connectors.LocalFS.TARGET_FILE_FORMAT : 'csv'}\n\nNewLocalfsDF = LocalfsDF.write.format(\"com.ibm.spark.discover\").options(**localfssaveoptions).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "## Load Data from IBM Biginsights HDFS"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "HDFSloadOptions = {\n    Connectors.HdfsBigInsights.URL : 'https://bi-hadoop-prod-4161.bi.services.us-south.bluemix.net:8443/gateway/default/webhdfs/v1/',\n    Connectors.HdfsBigInsights.USERNAME : '***********', \n    Connectors.HdfsBigInsights.PASSWORD : '***********',\n    Connectors.HdfsBigInsights.SOURCE_FILE_NAME : 'token.csv',\n    Connectors.HdfsBigInsights.SOURCE_FILE_FORMAT : 'csv',\n    Connectors.HdfsBigInsights.SOURCE_INFER_SCHEMA : '1'\n  }\n\nhdfsDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**HDFSloadOptions).load()\nhdfsDF.printSchema()\nhdfsDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "## Save the DataFrame to IBM Biginsights HDFS"
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": false
            }, 
            "source": "Once we have Analyzed and Transformed the Dataframe loaded as a part of load option above , User has the option to save the modified DataFrame back to HDFS using the below code . In our sample code we are assuming that above DataFrame hdfsDF is the new Transformed Dataset that we need to save it back to IBM Biginsights HDFS."
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "HDFSsaveOptions = {\n    Connectors.HdfsBigInsights.URL : 'https://bi-hadoop-prod-4161.bi.services.us-south.bluemix.net:8443/gateway/default/webhdfs/v1/',\n    Connectors.HdfsBigInsights.USERNAME : '***********', \n    Connectors.HdfsBigInsights.PASSWORD : '***********',\n    Connectors.HdfsBigInsights.TARGET_FILE_NAME : 'token2.csv',\n    Connectors.HdfsBigInsights.TARGET_WRITE_MODE : 'write'\n}\n\nNewHdfsDF = hdfsDF.write.format(\"com.ibm.spark.discover\").options(**HDFSsaveOptions).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load Data from Hive"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "HiveloadOptions = { Connectors.Hive.HOST                        : '***********',\n                      Connectors.Hive.PORT                      : '****',\n                      Connectors.Hive.DATABASE                  : 'BLUDB',\n                      Connectors.Hive.USERNAME                  : '***********',\n                      Connectors.Hive.PASSWORD                  : '***********',\n                      Connectors.Hive.SOURCE_TABLE_NAME         : '***********'}\n\nHiveDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**HiveloadOptions).load()\nHiveDF.printSchema()\nHiveDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load Data from Amazon Redshift"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "RedshiftloadOptions = { \n                     Connectors.Redshift.HOST              : '***********',\n                     Connectors.Redshift.PORT              : '***********',\n                     Connectors.Redshift.DATABASE          : 'BLUDB',\n                     Connectors.Redshift.USERNAME          : '***********',\n                     Connectors.Redshift.PASSWORD          : '***********',\n                      Connectors.Redshift.SOURCE_TABLE_NAME         : '***********'}\n\nRedshiftDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**RedshiftloadOptions).load()\nRedshiftDF.printSchema()\nRedshiftDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Amazon Redshift"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "Redshiftsaveoption = { \n                     Connectors.Redshift.HOST              : '***********',\n                     Connectors.Redshift.PORT              : '***********',\n                     Connectors.Redshift.DATABASE          : 'BLUDB',\n                     Connectors.Redshift.USERNAME          : '***********',\n                     Connectors.Redshift.PASSWORD          : '***********',\n                     Connectors.Redshift.TARGET_TABLE_NAME : 'DASH105036.TABLE2',\n                     Connectors.Redshift.TARGET_TABLE_ACTION : 'merge'}\n\nNewRedshiftDF = RedshiftDF.write.format(\"com.ibm.spark.discover\").options(**Redshiftsaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load Data from DB2"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "DB2loadOptions = { \n                     Connectors.DB2.HOST              : '***********',\n                     Connectors.DB2.PORT              : '***********',\n                     Connectors.DB2.DATABASE          : 'BLUDB',\n                     Connectors.DB2.USERNAME          : '***********',\n                     Connectors.DB2.PASSWORD          : '***********',\n                      Connectors.DB2.SOURCE_TABLE_NAME         : '***********'}\n\nDB2DF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**DB2loadOptions).load()\nDB2DF.printSchema()\nDB2DF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to DB2"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "DB2saveoption = { \n                     Connectors.DB2.HOST              : '***********',\n                     Connectors.DB2.PORT              : '***********',\n                     Connectors.DB2.DATABASE          : 'BLUDB',\n                     Connectors.DB2.USERNAME          : '***********',\n                     Connectors.DB2.PASSWORD          : '***********',\n                     Connectors.DB2.TARGET_TABLE_NAME : 'DASH105036.TABLE2',\n                     Connectors.DB2.TARGET_TABLE_ACTION : 'merge',\n                     Connectors.DB2.TARGET_WRITE_MODE : 'insert'}\n\nNewDB2DF = DB2DF.write.format(\"com.ibm.spark.discover\").options(**DB2saveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load Data from Informix"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "InformixloadOptions = { \n                     Connectors.Informix.HOST              : '***********',\n                     Connectors.Informix.PORT              : '***********',\n                     Connectors.Informix.SERVER            : '***********',\n                     Connectors.Informix.DATABASE          : 'BLUDB',\n                     Connectors.Informix.USERNAME          : '***********',\n                     Connectors.Informix.PASSWORD          : '***********',\n                      Connectors.Informix.SOURCE_TABLE_NAME         : '***********'}\n\nInformixDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**InformixloadOptions).load()\nInformixDF.printSchema()\nInformixDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Informix"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "Informixsaveoption = { \n                     Connectors.Informix.HOST              : '***********',\n                     Connectors.Informix.PORT              : '***********',\n                     Connectors.Informix.SERVER            : '***********',\n                     Connectors.Informix.DATABASE          : 'BLUDB',\n                     Connectors.Informix.USERNAME          : '***********',\n                     Connectors.Informix.PASSWORD          : '***********',\n                     Connectors.Informix.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.Informix.TARGET_TABLE_ACTION : 'merge'}\n\nNewInformixDF = InformixDF.write.format(\"com.ibm.spark.discover\").options(**Informixsaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from Watson Analytics"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "WatsonAnalyticsloadOptions = { \n                     Connectors.WatsonAnalytics.CLIENT_ID              : '***********',\n                     Connectors.WatsonAnalytics.SECRET_ID              : '***********',\n                     Connectors.WatsonAnalytics.CUSTOM_URL            : '***********',\n                     Connectors.WatsonAnalytics.USERNAME          : '***********',\n                     Connectors.WatsonAnalytics.PASSWORD          : '***********',\n                     Connectors.WatsonAnalytics.SOURCE_FILE_NAME         : '***********'}\n\nWatsonAnalyticsDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**WatsonAnalyticsloadOptions).load()\nWatsonAnalyticsDF.printSchema()\nWatsonAnalyticsDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Watson Analytics"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "WatsonAnalyticssaveoption = { \n                     Connectors.WatsonAnalytics.CLIENT_ID              : '***********',\n                     Connectors.WatsonAnalytics.SECRET_ID              : '***********',\n                     Connectors.WatsonAnalytics.CUSTOM_URL            : '***********',\n                     Connectors.WatsonAnalytics.USERNAME          : '***********',\n                     Connectors.WatsonAnalytics.PASSWORD          : '***********',\n                     Connectors.WatsonAnalytics.TARGET_FILE_NAME : '********',\n                     Connectors.WatsonAnalytics.TARGET_WA_META_DATA : '***********',\n                     Connectors.WatsonAnalytics.TARGET_WRITE_MODE : '*****'}\n\nNewWatsonAnalyticsDF = WatsonAnalyticsDF.write.format(\"com.ibm.spark.discover\").options(**WatsonAnalyticssaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from SQL Server"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [], 
            "source": "SqlServerloadOptions = { \n                     Connectors.SqlServer.HOST              : '***********',\n                     Connectors.SqlServer.PORT              : '***********',\n                     Connectors.SqlServer.DATABASE          : 'BLUDB',\n                     Connectors.SqlServer.USERNAME          : '***********',\n                     Connectors.SqlServer.PASSWORD          : '***********',\n                      Connectors.SqlServer.SOURCE_TABLE_NAME         : '***********'}\n\nSqlServerDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**SqlServerloadOptions).load()\nSqlServerDF.printSchema()\nSqlServerDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to SQL Server"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "SqlServersaveoption = { \n                     Connectors.SqlServer.HOST              : '***********',\n                     Connectors.SqlServer.PORT              : '***********',\n                     Connectors.SqlServer.DATABASE          : 'BLUDB',\n                     Connectors.SqlServer.USERNAME          : '***********',\n                     Connectors.SqlServer.PASSWORD          : '***********',\n                     Connectors.SqlServer.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.SqlServer.TARGET_TABLE_ACTION : 'merge'}\n\nNewSqlServerDF = SqlServerDF.write.format(\"com.ibm.spark.discover\").options(**SqlServersaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from MySql"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "MySQLloadOptions = { \n                     Connectors.MySQL.HOST              : '***********',\n                     Connectors.MySQL.PORT              : '***********',\n                     Connectors.MySQL.DATABASE          : 'BLUDB',\n                     Connectors.MySQL.USERNAME          : '***********',\n                     Connectors.MySQL.PASSWORD          : '***********',\n                      Connectors.MySQL.SOURCE_TABLE_NAME         : '***********'}\n\nMySQLDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**MySQLloadOptions).load()\nMySQLDF.printSchema()\nMySQLDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to MySQL"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "MySQLsaveoption = { \n                     Connectors.MySQL.HOST              : '***********',\n                     Connectors.MySQL.PORT              : '***********',\n                     Connectors.MySQL.DATABASE          : 'BLUDB',\n                     Connectors.MySQL.USERNAME          : '***********',\n                     Connectors.MySQL.PASSWORD          : '***********',\n                     Connectors.MySQL.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.MySQL.TARGET_TABLE_ACTION : 'merge'}\n\nNewMySQLDF = MySQLDF.write.format(\"com.ibm.spark.discover\").options(**MySQLsaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from Netezza"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "NetezzaloadOptions = { \n                     Connectors.Netezza.HOST              : '***********',\n                     Connectors.Netezza.PORT              : '***********',\n                     Connectors.Netezza.DATABASE          : 'BLUDB',\n                     Connectors.Netezza.USERNAME          : '***********',\n                     Connectors.Netezza.PASSWORD          : '***********',\n                      Connectors.Netezza.SOURCE_TABLE_NAME         : '***********'}\n\nNetezzaDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**NetezzaloadOptions).load()\nNetezzaDF.printSchema()\nNetezzaDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Netezza"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "Netezzasaveoption = { \n                     Connectors.Netezza.HOST              : '***********',\n                     Connectors.Netezza.PORT              : '***********',\n                     Connectors.Netezza.DATABASE          : 'BLUDB',\n                     Connectors.Netezza.USERNAME          : '***********',\n                     Connectors.Netezza.PASSWORD          : '***********',\n                     Connectors.Netezza.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.Netezza.TARGET_TABLE_ACTION : 'merge',\n                     Connectors.Netezza.TARGET_WRITE_MODE : 'insert'}\n\nNewNetezzaDF = NetezzaDF.write.format(\"com.ibm.spark.discover\").options(**Netezzasaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from Oracle"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "OracleloadOptions = { \n                     Connectors.Oracle.HOST              : '***********',\n                     Connectors.Oracle.PORT              : '***********',\n                     Connectors.Oracle.SID               : 'BLUDB',\n                     Connectors.Oracle.SERVICE_NAME      : '***********',\n                     Connectors.Oracle.USERNAME          : '***********',\n                     Connectors.Oracle.PASSWORD          : '***********',\n                      Connectors.Oracle.SOURCE_TABLE_NAME         : '***********'}\n\nOracleDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**OracleloadOptions).load()\nOracleDF.printSchema()\nOracleDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Oracle"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "Oraclesaveoption = { \n                     Connectors.Oracle.HOST              : '***********',\n                     Connectors.Oracle.PORT              : '***********',\n                     Connectors.Oracle.SID               : '***********',\n                     Connectors.Oracle.SERVICE_NAME      : '***********',\n                     Connectors.Oracle.USERNAME          : '***********',\n                     Connectors.Oracle.PASSWORD          : '***********',\n                     Connectors.Oracle.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.Oracle.TARGET_TABLE_ACTION : 'merge'}\n\nNewOracleDF = OracleDF.write.format(\"com.ibm.spark.discover\").options(**Oraclesaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from Greenplum"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "GreenplumloadOptions = { \n                     Connectors.Greenplum.HOST              : '***********',\n                     Connectors.Greenplum.PORT              : '***********',\n                     Connectors.Greenplum.DATABASE          : 'BLUDB',\n                     Connectors.Greenplum.USERNAME          : '***********',\n                     Connectors.Greenplum.PASSWORD          : '***********',\n                      Connectors.Greenplum.SOURCE_TABLE_NAME         : '***********'}\n\nGreenplumDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**GreenplumloadOptions).load()\nGreenplumDF.printSchema()\nGreenplumDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Greenplum"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "Greenplumsaveoption = { \n                     Connectors.Greenplum.HOST              : '***********',\n                     Connectors.Greenplum.PORT              : '***********',\n                     Connectors.Greenplum.DATABASE          : 'BLUDB',\n                     Connectors.Greenplum.USERNAME          : '***********',\n                     Connectors.Greenplum.PASSWORD          : '***********',\n                     Connectors.Greenplum.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.Greenplum.TARGET_TABLE_ACTION : 'merge'}\n\nNewGreenplumDF = GreenplumDF.write.format(\"com.ibm.spark.discover\").options(**Greenplumsaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from PostgreSQL"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "PostgreSQLloadOptions = { \n                     Connectors.PostgreSQL.HOST              : '***********',\n                     Connectors.PostgreSQL.PORT              : '***********',\n                     Connectors.PostgreSQL.DATABASE          : 'BLUDB',\n                     Connectors.PostgreSQL.USERNAME          : '***********',\n                     Connectors.PostgreSQL.PASSWORD          : '***********',\n                      Connectors.PostgreSQL.SOURCE_TABLE_NAME         : '***********'}\n\nPostgreSQLDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**PostgreSQLloadOptions).load()\nPostgreSQLDF.printSchema()\nPostgreSQLDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to PostgreSQL"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "PostgreSQLsaveoption = { \n                     Connectors.PostgreSQL.HOST              : '***********',\n                     Connectors.PostgreSQL.PORT              : '***********',\n                     Connectors.PostgreSQL.DATABASE          : 'BLUDB',\n                     Connectors.PostgreSQL.USERNAME          : '***********',\n                     Connectors.PostgreSQL.PASSWORD          : '***********',\n                     Connectors.PostgreSQL.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.PostgreSQL.TARGET_TABLE_ACTION : 'merge'}\n\nNewPostgreSQLDF = PostgreSQLDF.write.format(\"com.ibm.spark.discover\").options(**PostgreSQLsaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from PostgreSQL on Compose"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "PostgreSQLComposeloadOptions = { \n                     Connectors.PostgreSQLCompose.HOST              : '***********',\n                     Connectors.PostgreSQLCompose.PORT              : '***********',\n                     Connectors.PostgreSQLCompose.DATABASE          : 'BLUDB',\n                     Connectors.PostgreSQLCompose.USERNAME          : '***********',\n                     Connectors.PostgreSQLCompose.PASSWORD          : '***********',\n                      Connectors.PostgreSQLCompose.SOURCE_TABLE_NAME         : '***********'}\n\nPostgreSQLComposeDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**PostgreSQLComposeloadOptions).load()\nPostgreSQLComposeDF.printSchema()\nPostgreSQLComposeDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to PostgreSQL on Compose"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "PostgreSQLComposesaveoption = { \n                     Connectors.PostgreSQLCompose.HOST              : '***********',\n                     Connectors.PostgreSQLCompose.PORT              : '***********',\n                     Connectors.PostgreSQLCompose.DATABASE          : 'BLUDB',\n                     Connectors.PostgreSQLCompose.USERNAME          : '***********',\n                     Connectors.PostgreSQLCompose.PASSWORD          : '***********',\n                     Connectors.PostgreSQL.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.PostgreSQL.TARGET_WRITE_MODE : 'insert',\n                     Connectors.PostgreSQL.TARGET_TABLE_ACTION : 'append'}\n\nNewPostgreSQLComposeDF = PostgreSQLComposeDF.write.format(\"com.ibm.spark.discover\").options(**PostgreSQLComposesaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from Salesforce.com"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "SalesforceloadOptions = { \n                     Connectors.Salesforce.USERNAME          : '***********',\n                     Connectors.Salesforce.PASSWORD          : '***********',\n                      Connectors.Salesforce.SOURCE_TABLE_NAME         : '***********'}\n\nSalesforceDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**SalesforceloadOptions).load()\nSalesforceDF.printSchema()\nSalesforceDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Salesforce.com"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "Salesforcesaveoption = { \n                     Connectors.Salesforce.USERNAME          : '***********',\n                     Connectors.Salesforce.PASSWORD          : '***********',\n                     Connectors.Salesforce.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.Salesforce.TARGET_TABLE_ACTION : 'append'}\n\nNewSalesforceDF = SalesforceDF.write.format(\"com.ibm.spark.discover\").options(**Salesforcesaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from Sybase"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "SybaseloadOptions = { \n                     Connectors.Sybase.HOST              : '***********',\n                     Connectors.Sybase.PORT              : '***********',\n                     Connectors.Sybase.DATABASE          : 'BLUDB',\n                     Connectors.Sybase.USERNAME          : '***********',\n                     Connectors.Sybase.PASSWORD          : '***********',\n                      Connectors.Sybase.SOURCE_TABLE_NAME         : '***********'}\n\nSybaseDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**SybaseloadOptions).load()\nSybaseDF.printSchema()\nSybaseDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to Sybase"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "Sybasesaveoption = { \n                     Connectors.Sybase.HOST              : '***********',\n                     Connectors.Sybase.PORT              : '***********',\n                     Connectors.Sybase.DATABASE          : 'BLUDB',\n                     Connectors.Sybase.USERNAME          : '***********',\n                     Connectors.Sybase.PASSWORD          : '***********',\n                     Connectors.Sybase.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.Sybase.TARGET_TABLE_ACTION : 'append'}\n\nNewSybaseDF = SybaseDF.write.format(\"com.ibm.spark.discover\").options(**Sybasesaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from SybaseIQ"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "SybaseIQloadOptions = { \n                     Connectors.SybaseIQ.HOST              : '***********',\n                     Connectors.SybaseIQ.PORT              : '***********',\n                     Connectors.SybaseIQ.DATABASE          : 'BLUDB',\n                     Connectors.SybaseIQ.USERNAME          : '***********',\n                     Connectors.SybaseIQ.PASSWORD          : '***********',\n                      Connectors.SybaseIQ.SOURCE_TABLE_NAME         : '***********'}\n\nSybaseIQDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**SybaseIQloadOptions).load()\nSybaseIQDF.printSchema()\nSybaseIQDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to SybaseIQ"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "SybaseIQsaveoption = { \n                     Connectors.SybaseIQ.HOST              : '***********',\n                     Connectors.SybaseIQ.PORT              : '***********',\n                     Connectors.SybaseIQ.DATABASE          : 'BLUDB',\n                     Connectors.SybaseIQ.USERNAME          : '***********',\n                     Connectors.SybaseIQ.PASSWORD          : '***********',\n                     Connectors.SybaseIQ.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.SybaseIQ.TARGET_TABLE_ACTION : 'append'}\n\nNewSybaseIQDF = SybaseIQDF.write.format(\"com.ibm.spark.discover\").options(**SybaseIQsaveoption).save()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Load data from SQLDB"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "SQLDBloadOptions = { \n                     Connectors.SQLDB.HOST              : '***********',\n                     Connectors.SQLDB.PORT              : '***********',\n                     Connectors.SQLDB.DATABASE          : 'BLUDB',\n                     Connectors.SQLDB.USERNAME          : '***********',\n                     Connectors.SQLDB.PASSWORD          : '***********',\n                      Connectors.SQLDB.SOURCE_TABLE_NAME         : '***********'}\n\nSQLDBDF = sqlContext.read.format(\"com.ibm.spark.discover\").options(**SQLDBloadOptions).load()\nSQLDBDF.printSchema()\nSQLDBDF.show()", 
            "execution_count": null
        }, 
        {
            "cell_type": "markdown", 
            "metadata": {}, 
            "source": "# Save the DataFrame to SQLDB"
        }, 
        {
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "SQLDBsaveoption = { \n                     Connectors.SQLDB.HOST              : '***********',\n                     Connectors.SQLDB.PORT              : '***********',\n                     Connectors.SQLDB.DATABASE          : 'BLUDB',\n                     Connectors.SQLDB.USERNAME          : '***********',\n                     Connectors.SQLDB.PASSWORD          : '***********',\n                     Connectors.SQLDB.TARGET_TABLE_NAME : 'TABLE2',\n                     Connectors.SQLDB.TARGET_TABLE_ACTION : 'append'}\n\nNewSQLDBDF = SQLDBDF.write.format(\"com.ibm.spark.discover\").options(**SQLDBsaveoption).save()", 
            "execution_count": null
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6", 
            "name": "python2", 
            "language": "python"
        }, 
        "language_info": {
            "version": "2.7.11", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "name": "python", 
            "pygments_lexer": "ipython2"
        }
    }, 
    "nbformat_minor": 0
}